{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfdc01e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\python39\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\python39\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\python39\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\python39\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\python39\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n"
     ]
    }
   ],
   "source": [
    "pip install - -upgrade pip\n",
    "pip install docx2txt\n",
    "pip install PyPDF2\n",
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346d1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63723e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"C:/EDI SEM1/data-scientist-resume-example.pdf\"\n",
    "pdf = PdfFileReader(file_path)\n",
    "\n",
    "with open(\"resume5.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for page_num in range(pdf.numPages):\n",
    "        print('page: {0}'.format(page_num))\n",
    "        pageObj = pdf.getPage(page_num)\n",
    "\n",
    "        try:\n",
    "            txt = pageObj.extract_text()\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            f.write('page {0}\\n'.format(page_num+1))\n",
    "             \n",
    "            f.write(txt)\n",
    "    f.close()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49318a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd99670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page K A N D A C E L O U D O D A T A S C I E N T I S C O N T A C Mount E D U C A T I O Rutgers September 2011 April New S K I L L Python SQL Time Series Productionizing Recommendation Customer AWSW O R K E X P E R I E N C Data June 2018 Deployed recommendation engine production conditionally recommend menu item based past increasing average order size Implemented various time series forecasting technique predict surge lowering customer wait 10 Designed model pilot increase incentive peak increasing driver availability Led team 3 data scientist model ordering process unique reported made recommendation increase order output Data Spectrix Analytical March 2016 June Built customer attrition random forest model monthly retention 12 basis point client likely providing relevant product feature Coordinated product marketing team kind client interaction resulted maximized increasing conversion Partnered product team create recommendation engine Python improved length page user incremental annual Compiled analyzed data surrounding prototype saved Data April 2015 March Collaborated product manager perform cohort opportunity reduce pricing segment user boost yearly revenue Constructed operational reporting Tableau scheduling saving annual Implemented pricing experiment customer lifetime value reported monthly client service opted employee assigned client\n",
      "Data Identify valuable data source automate collection Undertake preprocessing structured unstructured Analyze large amount information discover trend Build predictive model Combine model ensemble Present information using data visualization Propose solution strategy business Collaborate engineering product development Requirements Proven experience Data Scientist Data Experience data Understanding operation Knowledge SQL familiarity Java Experience using business intelligence tool data framework Analytical mind business Strong math skill Excellent communication presentation 4 year degree Computer Science Information Technology preferable\n",
      "[[1.         0.24684618]\n",
      " [0.24684618 1.        ]]\n",
      "[1.         0.24684618]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "job_description = (\"C:\\EDI SEM1\\FullStack.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "f=open(\"resume5.txt\",\"r\",encoding=\"ISO-8859-1\") \n",
    "\n",
    "lst=[]\n",
    "\n",
    "for line in f:\n",
    "    for i in line.split(\" \"):\n",
    "        lst.append(i)\n",
    "lst=([lemmatizer.lemmatize(w) for w in lst])\n",
    "\n",
    "new_words= [word for word in lst if word.isalnum()]\n",
    " \n",
    "# print(new_words)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "WordSet = []\n",
    "for word in new_words:\n",
    "   if word not in set(stopwords.words(\"english\")):\n",
    "      WordSet.append(word)\n",
    "# print(WordSet)\n",
    "\n",
    "string=\" \".join(WordSet)\n",
    "\n",
    "\n",
    "f=open(\"C:/EDI SEM1/job_desciptions/datascientist.txt\",\"r\",encoding=\"ISO-8859-1\") \n",
    "\n",
    "lst=[]\n",
    "\n",
    "for line in f:\n",
    "    for i in line.split(\" \"):\n",
    "        lst.append(i)\n",
    "lst=([lemmatizer.lemmatize(w) for w in lst])\n",
    "\n",
    "new_words= [word for word in lst if word.isalnum()]\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "WordSet = []\n",
    "for word in new_words:\n",
    "   if word not in set(stopwords.words(\"english\")):\n",
    "      WordSet.append(word)\n",
    "# print(WordSet)\n",
    "\n",
    "string2=\" \".join(WordSet)\n",
    "print(string)\n",
    "print(string2)\n",
    "\n",
    "\n",
    "content = [string2,string]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "matrix = cv.fit_transform(content)\n",
    "\n",
    "\n",
    "similarity_matrix = cosine_similarity(matrix)\n",
    "print(similarity_matrix)\n",
    "\n",
    "print(similarity_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2dc026",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\EDI SEM1\\resume\\Untitled.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/EDI%20SEM1/resume/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m content \u001b[39m=\u001b[39m [job_description, new_words]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/EDI%20SEM1/resume/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m cv \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/EDI%20SEM1/resume/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m matrix \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mfit_transform(content)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/EDI%20SEM1/resume/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(matrix)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/EDI%20SEM1/resume/Untitled.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(similarity_matrix)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[39m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \n\u001b[0;32m   2060\u001b[0m \u001b[39mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m-> 2077\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2079\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1202\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "job_description = (\"C:\\EDI SEM1\\machine_learning.txt\")\n",
    "resume = (\"C:/EDI SEM1/resume23.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "content = [job_description, new_words]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "matrix = cv.fit_transform(content)\n",
    "\n",
    "\n",
    "similarity_matrix = cosine_similarity(matrix)\n",
    "print(similarity_matrix)\n",
    "\n",
    "print(similarity_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7eb727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53c9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63cc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac499b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8c5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "996f2b6e98019c29df53ccff772a8e3fe7cdecb0ba4b3f6ddb9be804d297dd3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
